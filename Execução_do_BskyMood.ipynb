{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXbqIHiGi/T03plgCdLjsA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobsrobson/Streamlit-Bsky/blob/main/Execu%C3%A7%C3%A3o_do_BskyMood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Execu√ß√£o do BskyMood via Google Colab"
      ],
      "metadata": {
        "id": "sIocBIkbRevV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vis√£o Geral\n",
        "Este notebook serve como ambiente de execu√ß√£o para o aplicativo **BskyMood**, uma ferramenta de coleta e an√°lise de sentimentos e t√≥picos da rede social Bluesky, constru√≠da com Streamlit.\n",
        "\n",
        "Devido √†s exig√™ncias computacionais dos modelos de Machine Learning utilizados, a execu√ß√£o do aplicativo na nuvem gratuita do Streamlit (Streamlit Community Cloud) torna-se invi√°vel. Adotei, portanto, uma abordagem alternativa que combina o poder computacional do Google Colab com a praticidade do `localtunnel` para expor o aplicativo na internet."
      ],
      "metadata": {
        "id": "KbqmOON9Rruw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Motiva√ß√£o\n",
        "\n",
        "A plataforma Streamlit Community Cloud √© fant√°stica para hospedar a maioria dos aplicativos, mas possui limita√ß√µes de recursos no seu plano gratuito (tipicamente 1 GB de RAM e poder de CPU restrito). O aplicativo ultrapassa esses limites por duas raz√µes principais:\n",
        "\n",
        "1. **An√°lise de Sentimentos com Transformers:** Carregamos um modelo de linguagem da Hugging Face (lxyuan/distilbert-base-multilingual-cased-sentiments-student) para realizar a an√°lise de sentimentos. Esses modelos, mesmo os \"distilados\", consomem uma quantidade significativa de mem√≥ria RAM para serem carregados e processados.\n",
        "\n",
        "2. **Modelagem de T√≥picos com BERTopic:** A biblioteca BERTopic √© computacionalmente intensiva. O processo envolve a cria√ß√£o de embeddings de alta dimensionalidade para todos os posts, a redu√ß√£o dessa dimensionalidade (usando UMAP) e a clusteriza√ß√£o (usando HDBSCAN). Essas etapas demandam um uso consider√°vel de CPU e RAM, especialmente √† medida que o n√∫mero de posts coletados aumenta.\n",
        "\n",
        "Solu√ß√£o: O Google Colab oferece um ambiente com recursos muito mais robustos (geralmente mais de 12 GB de RAM e acesso a GPUs), permitindo que o aplicativo execute essas tarefas pesadas sem falhar."
      ],
      "metadata": {
        "id": "DzjhDgi1SLeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Como Funciona?\n",
        "\n",
        "Ao executar um aplicativo Streamlit em um notebook do Colab, ele roda em um servidor web local dentro da m√°quina virtual do Colab. Por padr√£o, esse servidor n√£o √© acess√≠vel pela internet p√∫blica. √â aqui que o `localtunnel` entra em a√ß√£o.\n",
        "\n",
        "`localtunnel` √© uma ferramenta que exp√µe seu servidor web local √† internet de forma segura. Ele funciona da seguinte maneira:\n",
        "\n",
        "- O aplicativo Streamlit √© iniciado e fica \"escutando\" em uma porta local (ex: porta 8501) dentro do ambiente do Colab.\n",
        "- Executamos o localtunnel, apontando para essa porta.\n",
        "O localtunnel cria uma URL p√∫blica √∫nica (ex: https://seu-nome-aleatorio.loca.lt).\n",
        "- Qualquer pessoa que acesse essa URL p√∫blica ter√° sua requisi√ß√£o \"tunelada\" de forma segura diretamente para o nosso aplicativo Streamlit rodando no Colab."
      ],
      "metadata": {
        "id": "ChLNYtNES6J1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Execute todas as c√©lulas abaixo, em ordem.**"
      ],
      "metadata": {
        "id": "4ll63M7HTLa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instala√ß√£o das Bibliotecas Necess√°rias (Python)\n",
        "!pip install streamlit pandas atproto regex langdetect transformers torch emoji bertopic scikit-learn nltk -q"
      ],
      "metadata": {
        "id": "zWvFf6nVRefa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjoaJdNsQ-aH"
      },
      "outputs": [],
      "source": [
        "# 2. Instala√ß√£o do localtunnel (NPM)\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Obten√ß√£o da senha de acesso ao localtunnel exposto\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "# COPIE E COLE ESTA SENHA QUANDO FOR SOLICITADO"
      ],
      "metadata": {
        "id": "IIcuolOhUHKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Grava√ß√£o do BskyMood em um arquivo .py dentro do armazenamento do Colab\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from atproto import FirehoseSubscribeReposClient, parse_subscribe_repos_message, CAR, IdResolver, DidInMemoryCache\n",
        "import time\n",
        "import multiprocessing\n",
        "import threading\n",
        "import regex as re\n",
        "from langdetect import detect\n",
        "import queue\n",
        "from datetime import datetime\n",
        "from transformers import pipeline\n",
        "import emoji\n",
        "\n",
        "# Novas importa√ß√µes para BERTopic\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "\n",
        "# Download das Stopwords do NLTK\n",
        "try:\n",
        "  nltk.download('stopwords', quiet=True)\n",
        "except Exception as e:\n",
        "  # N√£o bloquear a UI se o download falhar, BERTopic pode funcionar sem, ou o usu√°rio pode instalar manualmente.\n",
        "  st.toast(f\"Alerta: N√£o foi poss√≠vel baixar stopwords do NLTK: {e}. A modelagem de t√≥picos ir√° prosseguir com as configura√ß√µes padr√£o do BERTopic.\", icon=\"‚ö†Ô∏è\")\n",
        "  print(f\"Alerta: N√£o foi poss√≠vel baixar stopwords do NLTK: {e}. A modelagem de t√≥picos ir√° prosseguir com as configura√ß√µes padr√£o do BERTopic.\")\n",
        "\n",
        "class BskyDataCollectorApp:\n",
        "    def __init__(self):\n",
        "        st.set_page_config(page_title='BskyMood', layout='wide')\n",
        "        self.svg_path_data = \"\"\"\n",
        "            M13.873 3.805C21.21 9.332 29.103 20.537 32 26.55v15.882c0-.338-.13.044-.41.867-1.512 4.456-7.418 21.847-20.923 7.944-7.111-7.32-3.819-14.64 9.125-16.85-7.405 1.264-15.73-.825-18.014-9.015C1.12 23.022 0 8.51 0 6.55 0-3.268 8.579-.182 13.873 3.805ZM50.127 3.805C42.79 9.332 34.897 20.537 32 26.55v15.882c0-.338.13.044.41.867 1.512 4.456 7.418 21.847 20.923 7.944 7.111-7.32 3.819-14.64-9.125-16.85 7.405 1.264 15.73-.825 18.014-9.015C62.88 23.022 64 8.51 64 6.55c0-9.818-8.578-6.732-13.873-2.745Z\n",
        "        \"\"\"\n",
        "        self.bskylogo_svg_template = f\"\"\"\n",
        "            <svg width=\"28\" height=\"28\" viewBox=\"0 0 64 64\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "                <path d=\"{self.svg_path_data}\" fill=\"#0085ff\"/>\n",
        "            </svg>\n",
        "        \"\"\"\n",
        "        self._initialize_session_state()\n",
        "        self._initialize_topic_session_state()\n",
        "        self.resolver_cache = DidInMemoryCache()\n",
        "        self.sentiment_pipeline = None\n",
        "        self.topic_model = None\n",
        "\n",
        "    def _initialize_session_state(self):\n",
        "        if 'data' not in st.session_state:\n",
        "            st.session_state['data'] = []\n",
        "        if 'collecting' not in st.session_state:\n",
        "            st.session_state['collecting'] = False\n",
        "        if 'collection_ended' not in st.session_state:\n",
        "            st.session_state['collection_ended'] = False\n",
        "        if 'stop_event' not in st.session_state:\n",
        "            st.session_state['stop_event'] = multiprocessing.Event()\n",
        "        if 'start_time' not in st.session_state:\n",
        "            st.session_state['start_time'] = 0.0\n",
        "        if 'data_queue' not in st.session_state:\n",
        "            st.session_state['data_queue'] = multiprocessing.Queue()\n",
        "        if 'sentiment_results' not in st.session_state:\n",
        "            st.session_state['sentiment_results'] = []\n",
        "        if 'collected_df' not in st.session_state:\n",
        "            st.session_state['collected_df'] = pd.DataFrame()\n",
        "\n",
        "    def _initialize_topic_session_state(self):\n",
        "        if 'topic_model_instance' not in st.session_state:\n",
        "            st.session_state['topic_model_instance'] = None\n",
        "        if 'topic_info_df' not in st.session_state:\n",
        "            st.session_state['topic_info_df'] = pd.DataFrame()\n",
        "        if 'topics_analyzed' not in st.session_state:\n",
        "            st.session_state['topics_analyzed'] = False\n",
        "        if 'performing_topic_analysis' not in st.session_state:\n",
        "            st.session_state['performing_topic_analysis'] = False\n",
        "        if 'texts_for_topic_analysis' not in st.session_state:\n",
        "            st.session_state['texts_for_topic_analysis'] = []\n",
        "\n",
        "    def _process_message(self, message, data_queue):\n",
        "        try:\n",
        "            commit = parse_subscribe_repos_message(message)\n",
        "            if not hasattr(commit, 'ops'):\n",
        "                return\n",
        "\n",
        "            for op in commit.ops:\n",
        "                if op.action == 'create' and op.path.startswith('app.bsky.feed.post/'):\n",
        "                    post_data = self._extract_post_data(commit, op)\n",
        "                    if post_data and self._lang_selector(post_data['text']):\n",
        "                        data_queue.put(post_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing message in thread: {e}\")\n",
        "\n",
        "    def _lang_selector(self, text):\n",
        "        try:\n",
        "            lang = detect(text)\n",
        "            return lang in ['en', 'pt', 'es']\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _extract_post_data(self, commit, op):\n",
        "        try:\n",
        "            car = CAR.from_bytes(commit.blocks)\n",
        "            author_handle = commit.repo\n",
        "\n",
        "            for record in car.blocks.values():\n",
        "                if isinstance(record, dict) and record.get('$type') == 'app.bsky.feed.post':\n",
        "                    return {\n",
        "                        'text': record.get('text', ''),\n",
        "                        'created_at': record.get('createdAt', ''),\n",
        "                        'author': author_handle,\n",
        "                        'uri': f'at://{commit.repo}/{op.path}',\n",
        "                        'has_images': 'embed' in record,\n",
        "                        'reply_to': record.get('reply', {}).get('parent', {}).get('uri')\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            st.toast(f\"Erro ao extrair dados: {e}\", icon=\":material/dangerous:\")\n",
        "            return None\n",
        "\n",
        "    def _collect_messages_threaded(self, stop_event, data_queue):\n",
        "        client = FirehoseSubscribeReposClient()\n",
        "        try:\n",
        "            client.start(lambda message: self._process_message(message, data_queue))\n",
        "        except Exception as e:\n",
        "            st.toast(f\"Erro na thread de coleta: {e}\", icon=\":material/dangerous:\")\n",
        "        finally:\n",
        "            client.stop()\n",
        "\n",
        "    def collect_data(self):\n",
        "        stop_event = st.session_state['stop_event']\n",
        "        data_queue = st.session_state['data_queue']\n",
        "        st.session_state['collection_ended'] = False\n",
        "\n",
        "        if st.session_state['collecting'] and not st.session_state['collection_ended']:\n",
        "            stop_button_pressed = st.button(\"Parar Coleta\", icon=\":material/stop_circle:\", help=\"Clique para parar a coleta de dados. Os dados j√° coletados ser√£o mantidos na mem√≥ria.\")\n",
        "            if stop_button_pressed:\n",
        "                st.session_state['stop_event'].set()\n",
        "                st.session_state['collecting'] = False\n",
        "\n",
        "        start_time = time.time()\n",
        "        collection_duration = st.session_state.get('collection_duration', 30)\n",
        "        collecting_data_flag = st.session_state['collecting']\n",
        "\n",
        "        if collecting_data_flag:\n",
        "            collection_thread = threading.Thread(target=self._collect_messages_threaded, args=(stop_event, data_queue))\n",
        "            collection_thread.daemon = True\n",
        "            collection_thread.start()\n",
        "\n",
        "            with st.status(f\"Coletando posts do Bluesky durante {collection_duration} segundos. Aguarde!\") as status:\n",
        "                mensagens = [\n",
        "                    \"Estabelecendo conex√£o com o Firehose...\",\n",
        "                    \"Conex√£o estabelecida com sucesso!\",\n",
        "                    \"Autenticando...\",\n",
        "                    \"Autentica√ß√£o conclu√≠da!\",\n",
        "                    \"Organizando a fila...\",\n",
        "                    \"Atualizando lista...\",\n",
        "                    \"Coletando posts... Isso pode demorar alguns minutos.\",\n",
        "                ]\n",
        "                for i, msg in enumerate(mensagens):\n",
        "                    status.update(label=msg)\n",
        "                    if i < len(mensagens) - 1:\n",
        "                        time.sleep(1 if i != 0 else 2)\n",
        "                        if stop_event.is_set() or (time.time() - start_time >= collection_duration):\n",
        "                            break\n",
        "                    else:\n",
        "                        while collecting_data_flag and not stop_event.is_set() and (time.time() - start_time < collection_duration):\n",
        "                            try:\n",
        "                                while not data_queue.empty():\n",
        "                                    st.session_state['data'].append(data_queue.get_nowait())\n",
        "                            except queue.Empty:\n",
        "                                pass\n",
        "                            except Exception as e:\n",
        "                                print(f\"Erro ao recuperar da fila: {e}\")\n",
        "                            time.sleep(0.5)\n",
        "                            collecting_data_flag = st.session_state['collecting']\n",
        "                            if not collecting_data_flag:\n",
        "                                stop_event.set()\n",
        "                try:\n",
        "                    while not data_queue.empty():\n",
        "                        st.session_state['data'].append(data_queue.get_nowait())\n",
        "                except queue.Empty:\n",
        "                    pass\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao recuperar dados restantes da fila: {e}\")\n",
        "\n",
        "            st.session_state['collecting'] = False\n",
        "            st.session_state['collection_ended'] = True\n",
        "            stop_event.set()\n",
        "            if collection_thread.is_alive():\n",
        "                collection_thread.join(timeout=2)\n",
        "            st.rerun()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        if not isinstance(text, str):\n",
        "            return ''\n",
        "\n",
        "        text = re.sub(r'@\\S+', '', text)\n",
        "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "        text = re.sub(r'www\\.\\S+', '', text)\n",
        "        text = re.sub(r'\\b[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}\\b(/\\S*)?', '', text, flags=re.IGNORECASE)\n",
        "        text = emoji.demojize(text, language='en')\n",
        "        return text\n",
        "\n",
        "    def analyze_sentiment(self, status_obj):\n",
        "        if not self.sentiment_pipeline:\n",
        "            try:\n",
        "                status_obj.update(label=\"Carregando modelo de an√°lise de sentimentos...\")\n",
        "                self.sentiment_pipeline = pipeline(\n",
        "                    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
        "                    return_all_scores=False,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                st.error(f\"Erro ao carregar o modelo de an√°lise de sentimentos: {e}\", icon=\":material/error:\")\n",
        "                status_obj.update(label=\"Falha ao carregar modelo de an√°lise.\", state=\"error\", expanded=True)\n",
        "                return\n",
        "\n",
        "        st.session_state['sentiment_results'] = []\n",
        "        updated_data_with_sentiment = []\n",
        "        if st.session_state['collection_ended'] and st.session_state['data']:\n",
        "            total_posts = len(st.session_state['data'])\n",
        "            for i, post in enumerate(st.session_state['data']):\n",
        "                status_obj.update(label=f\"Analisando post {i+1}/{total_posts}: \\\"{post['text'][:50]}...\\\"\")\n",
        "                try:\n",
        "                    processed_text = self.preprocess_text(post['text'])\n",
        "                    if not processed_text.strip():\n",
        "                        sentiment = \"neutral\"\n",
        "                    else:\n",
        "                        result = self.sentiment_pipeline(processed_text)[0]\n",
        "                        sentiment = result['label']\n",
        "\n",
        "                    post_with_sentiment = post.copy()\n",
        "                    post_with_sentiment['sentiment'] = sentiment\n",
        "                    updated_data_with_sentiment.append(post_with_sentiment)\n",
        "                    st.session_state['sentiment_results'].append({'text': post['text'], 'sentiment': sentiment})\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Erro ao analisar o sentimento do post \\\"{post['text'][:50]}...\\\": {e}\", icon=\":material/error:\")\n",
        "                    post_with_error = post.copy()\n",
        "                    post_with_error['sentiment'] = 'analysis_error'\n",
        "                    updated_data_with_sentiment.append(post_with_error)\n",
        "\n",
        "            status_obj.update(label=\"An√°lise de sentimentos conclu√≠da!\", state=\"complete\", expanded=False)\n",
        "            st.session_state['data'] = updated_data_with_sentiment\n",
        "        else:\n",
        "            st.error(\"N√£o h√° dados coletados para an√°lise de sentimentos.\", icon=\":material/error:\")\n",
        "            status_obj.update(label=\"Nenhum dado para analisar.\", state=\"error\", expanded=True)\n",
        "            return\n",
        "\n",
        "    def perform_topic_modeling_and_sentiment(self, status_obj):\n",
        "        st.session_state['performing_topic_analysis'] = True\n",
        "        st.session_state['topics_analyzed'] = False\n",
        "\n",
        "        if not st.session_state.get('data'):\n",
        "            st.warning(\"N√£o h√° dados coletados para a an√°lise de t√≥picos.\", icon=\"‚ö†Ô∏è\")\n",
        "            status_obj.update(label=\"Nenhum dado para an√°lise de t√≥picos.\", state=\"error\", expanded=True)\n",
        "            st.session_state['performing_topic_analysis'] = False\n",
        "            return\n",
        "\n",
        "        status_obj.update(label=\"Preparando textos para modelagem de t√≥picos...\")\n",
        "        texts_for_bertopic = [self.preprocess_text(post.get('text', '')) for post in st.session_state['data']]\n",
        "        st.session_state['texts_for_topic_analysis'] = texts_for_bertopic\n",
        "\n",
        "        if not any(texts_for_bertopic):\n",
        "            st.warning(\"Nenhum texto v√°lido encontrado nos posts para a an√°lise de t√≥picos ap√≥s o pr√©-processamento.\", icon=\"‚ö†Ô∏è\")\n",
        "            status_obj.update(label=\"Nenhum texto para modelagem de t√≥picos.\", state=\"error\", expanded=True)\n",
        "            st.session_state['performing_topic_analysis'] = False\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            stop_words_en = list(nltk.corpus.stopwords.words('english'))\n",
        "            stop_words_pt = list(nltk.corpus.stopwords.words('portuguese'))\n",
        "            stop_words_es = list(nltk.corpus.stopwords.words('spanish'))\n",
        "            all_stop_words = stop_words_en + stop_words_pt + stop_words_es\n",
        "            vectorizer_model = CountVectorizer(stop_words=all_stop_words)\n",
        "        except Exception as e:\n",
        "            st.warning(f\"N√£o foi poss√≠vel carregar stopwords do NLTK: {e}. Usando BERTopic com configura√ß√µes padr√£o de idioma.\", icon=\"‚ö†Ô∏è\")\n",
        "            vectorizer_model = None\n",
        "\n",
        "        try:\n",
        "            status_obj.update(label=\"Iniciando modelagem de t√≥picos com BERTopic... Isso pode levar alguns minutos.\")\n",
        "            if vectorizer_model:\n",
        "                 self.topic_model = BERTopic(language=\"multilingual\",\n",
        "                                            vectorizer_model=vectorizer_model,\n",
        "                                            min_topic_size=3,\n",
        "                                            verbose=True)\n",
        "            else:\n",
        "                self.topic_model = BERTopic(language=\"multilingual\",\n",
        "                                            min_topic_size=3,\n",
        "                                            verbose=True)\n",
        "\n",
        "            topics, probabilities = self.topic_model.fit_transform(texts_for_bertopic)\n",
        "            st.session_state['topic_model_instance'] = self.topic_model\n",
        "\n",
        "            status_obj.update(label=\"Modelagem de t√≥picos conclu√≠da. Processando resultados...\")\n",
        "\n",
        "            if len(st.session_state['data']) == len(topics):\n",
        "                for i, post_data in enumerate(st.session_state['data']):\n",
        "                    post_data['topic_id'] = topics[i]\n",
        "            else:\n",
        "                st.warning(\"Inconsist√™ncia no n√∫mero de posts e resultados de t√≥picos. N√£o foi poss√≠vel adicionar topic_id aos dados.\")\n",
        "\n",
        "            topic_info_df = self.topic_model.get_topic_info()\n",
        "\n",
        "            posts_df_for_topic_sentiment = pd.DataFrame(st.session_state['data'])\n",
        "\n",
        "            if 'sentiment' in posts_df_for_topic_sentiment.columns and 'topic_id' in posts_df_for_topic_sentiment.columns:\n",
        "                status_obj.update(label=\"Analisando sentimentos por t√≥pico...\")\n",
        "                sentiment_by_topic = posts_df_for_topic_sentiment[posts_df_for_topic_sentiment['topic_id'] != -1] \\\n",
        "                                     .groupby('topic_id')['sentiment'] \\\n",
        "                                     .value_counts(normalize=True) \\\n",
        "                                     .unstack(fill_value=0)\n",
        "\n",
        "                sentiment_by_topic = sentiment_by_topic.rename(columns={\n",
        "                    'positive': 'Positive (%)',\n",
        "                    'negative': 'Negative (%)',\n",
        "                    'neutral': 'Neutral (%)',\n",
        "                    'analysis_error': 'Error (%)'\n",
        "                })\n",
        "\n",
        "                for col_name in ['Positive (%)', 'Negative (%)', 'Neutral (%)', 'Error (%)']:\n",
        "                    if col_name in sentiment_by_topic.columns:\n",
        "                        sentiment_by_topic[col_name] = (sentiment_by_topic[col_name] * 100).round(1)\n",
        "\n",
        "                if 'Topic' in topic_info_df.columns:\n",
        "                    topic_info_df = topic_info_df.merge(sentiment_by_topic, left_on='Topic', right_index=True, how='left')\n",
        "                    topic_info_df.fillna(0, inplace=True)\n",
        "                else:\n",
        "                    st.warning(\"Coluna 'Topic' n√£o encontrada no DataFrame de informa√ß√µes do t√≥pico. N√£o foi poss√≠vel mesclar com os sentimentos por t√≥pico.\", icon=\"‚ö†Ô∏è\")\n",
        "            else:\n",
        "                st.warning(\"Coluna 'sentiment' ou 'topic_id' n√£o encontrada nos dados dos posts. N√£o foi poss√≠vel realizar a an√°lise de sentimento por t√≥pico.\", icon=\"‚ö†Ô∏è\")\n",
        "\n",
        "            st.session_state['topic_info_df'] = topic_info_df\n",
        "            st.session_state['topics_analyzed'] = True\n",
        "            status_obj.update(label=\"An√°lise de t√≥picos e sentimentos por t√≥pico conclu√≠da!\", state=\"complete\", expanded=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erro durante a modelagem de t√≥picos: {e}\", icon=\":material/error:\")\n",
        "            status_obj.update(label=f\"Erro na modelagem de t√≥picos: {e}\", state=\"error\", expanded=True)\n",
        "        finally:\n",
        "            st.session_state['performing_topic_analysis'] = False\n",
        "\n",
        "\n",
        "    def display_data(self):\n",
        "        if len(st.session_state['data']) > 0:\n",
        "            df_collected = pd.DataFrame(st.session_state['data'])\n",
        "            st.session_state['collected_df'] = df_collected\n",
        "            num_rows = len(df_collected)\n",
        "\n",
        "            num_has_images = df_collected['has_images'].sum() if 'has_images' in df_collected.columns else 0\n",
        "            num_is_reply = df_collected['reply_to'].notna().sum() if 'reply_to' in df_collected.columns else 0\n",
        "\n",
        "            if st.session_state['collection_ended'] and not st.session_state.get('performing_topic_analysis', False) and not st.session_state.get('collecting', False) :\n",
        "                if not st.session_state.get('topics_analyzed_toast_shown', False) and not st.session_state.get('sentiment_analysis_toast_shown', False):\n",
        "                     st.toast(f\"A√ß√£o finalizada com sucesso!\", icon=\":material/check_circle:\")\n",
        "\n",
        "            if 'sentiment' in df_collected.columns and st.session_state.get('sentiment_results') and not st.session_state.get('topics_analyzed'):\n",
        "                total_analyzed = len(df_collected)\n",
        "                sentiment_counts = df_collected['sentiment'].value_counts()\n",
        "                positive_count = sentiment_counts.get('positive', 0)\n",
        "                negative_count = sentiment_counts.get('negative', 0)\n",
        "                neutral_count = sentiment_counts.get('neutral', 0)\n",
        "                positive_percentage = (positive_count / total_analyzed) * 100 if total_analyzed > 0 else 0\n",
        "                negative_percentage = (negative_count / total_analyzed) * 100 if total_analyzed > 0 else 0\n",
        "                neutral_percentage = (neutral_count / total_analyzed) * 100 if total_analyzed > 0 else 0\n",
        "\n",
        "                col_metric1, col_metric2, col_metric3, col_metric4 = st.columns(4, gap=\"small\")\n",
        "                with col_metric1:\n",
        "                    st.metric(label=\"Total de Posts Analisados\", value=total_analyzed)\n",
        "                with col_metric2:\n",
        "                    st.metric(label=\"Posts Positivos\", value=f\"{positive_percentage:.1f}%\")\n",
        "                with col_metric3:\n",
        "                    st.metric(label=\"Posts Negativos\", value=f\"{negative_percentage:.1f}%\")\n",
        "                with col_metric4:\n",
        "                    st.metric(label=\"Posts Neutros\", value=f\"{neutral_percentage:.1f}%\")\n",
        "            elif not st.session_state.get('topics_analyzed'):\n",
        "                col1_metrics, col2_metrics, col3_metrics = st.columns(3, gap=\"small\")\n",
        "                with col1_metrics:\n",
        "                    st.metric(label=\"Total de Posts Coletados\", value=num_rows)\n",
        "                with col2_metrics:\n",
        "                    st.metric(label=\"Posts com Imagens\", value=num_has_images)\n",
        "                with col3_metrics:\n",
        "                    st.metric(label=\"Posts em Reply\", value=num_is_reply)\n",
        "\n",
        "            st.session_state['collected_df_for_download'] = df_collected\n",
        "\n",
        "            if 'sentiment' in df_collected.columns and st.session_state.get('sentiment_results') and not st.session_state.get('topics_analyzed', False):\n",
        "                st.subheader(\"Resultados da An√°lise de Sentimentos Individual\")\n",
        "                st.sidebar.title(\"\")\n",
        "                st.sidebar.warning(\n",
        "                    \"- A an√°lise ainda n√£o √© 100% precisa. Erros de classifica√ß√£o podem ocorrer em algumas postagens.\\n\"\n",
        "                    \"- A an√°lise de sentimentos √© realizada automaticamente e pode n√£o refletir a inten√ß√£o original do autor da postagem.\\n\"\n",
        "                    \"- Men√ß√µes e URLs s√£o removidos durante a an√°lise, mas ainda s√£o exibidos na tabela para fins de registro.\\n\"\n",
        "                    \"- As postagens podem incluir termos ofensivos ou inadequados, pois n√£o h√° filtragem de conte√∫do.\"\n",
        "                )\n",
        "                columns_to_show = ['text', 'sentiment', 'topic_id'] if 'topic_id' in df_collected.columns else ['text', 'sentiment']\n",
        "                available_columns = [col for col in columns_to_show if col in df_collected.columns]\n",
        "                st.dataframe(df_collected[available_columns], use_container_width=True)\n",
        "            elif not df_collected.empty and not st.session_state.get('topics_analyzed', False) :\n",
        "                st.subheader(\"Dados Coletados\")\n",
        "                st.sidebar.warning(\n",
        "                    \"- Para executar a an√°lise de sentimentos individuais, clique em 'Analisar Sentimentos'.\\n\"\n",
        "                    \"- Para executar a an√°lise de t√≥picos, conclua a an√°lise de sentimentos primeiro.\\n\"\n",
        "                    \"- Aten√ß√£o: as an√°lises podem levar v√°rios minutos.\\n\"\n",
        "                    \"- As postagens podem incluir termos ofensivos ou inadequados, pois n√£o h√° filtragem de conte√∫do.\\n\",\n",
        "                )\n",
        "                cols_to_display = ['text', 'created_at', 'author', 'has_images', 'reply_to']\n",
        "                if 'topic_id' in df_collected.columns:\n",
        "                    cols_to_display.append('topic_id')\n",
        "\n",
        "                cols_to_display_existing = [col for col in cols_to_display if col in df_collected.columns]\n",
        "                st.dataframe(df_collected[cols_to_display_existing], use_container_width=True)\n",
        "\n",
        "\n",
        "            col1_buttons, col2_buttons, col3_buttons, col4_buttons = st.columns([1.7, 1.7, 1, 1])\n",
        "            status_container_sentiment = st.empty()\n",
        "            status_container_topics = st.empty()\n",
        "\n",
        "            with col1_buttons:\n",
        "                if not st.session_state.get('sentiment_results') and 'sentiment' not in df_collected.columns:\n",
        "                    if st.button(\"Analisar Sentimentos\", icon=\":material/psychology:\", use_container_width=True, type=\"primary\", help=\"Clique para analisar os sentimentos dos posts coletados individualmente.\"):\n",
        "                        with status_container_sentiment.status(\"Preparando o ambiente para a an√°lise de sentimentos individuais...\", expanded=True) as status:\n",
        "                            self.analyze_sentiment(status)\n",
        "                        st.session_state['sentiment_analysis_toast_shown'] = True\n",
        "                        st.rerun()\n",
        "                elif 'sentiment' in df_collected.columns:\n",
        "                     st.button(\"Analisar Sentimentos\", icon=\":material/psychology:\", use_container_width=True, type=\"primary\", help=\"Sentimentos individuais j√° analisados.\", disabled=True)\n",
        "\n",
        "            with col2_buttons:\n",
        "                # *** IN√çCIO DA MODIFICA√á√ÉO: L√≥gica para desabilitar o bot√£o \"Analisar T√≥picos\" ***\n",
        "                sentiment_analysis_done = 'sentiment' in df_collected.columns\n",
        "                topics_already_analyzed = st.session_state.get('topics_analyzed', False)\n",
        "\n",
        "                # Definir a condi√ß√£o para desabilitar o bot√£o\n",
        "                disable_topic_button = topics_already_analyzed or not sentiment_analysis_done\n",
        "\n",
        "                # Definir a mensagem de ajuda (tooltip) com base no motivo do bloqueio\n",
        "                if topics_already_analyzed:\n",
        "                    help_text = \"T√≥picos j√° analisados.\"\n",
        "                elif not sentiment_analysis_done:\n",
        "                    help_text = \"Execute a 'An√°lise de Sentimentos' primeiro.\"\n",
        "                else:\n",
        "                    help_text = \"Clique para extrair t√≥picos e analisar sentimentos por t√≥pico.\"\n",
        "\n",
        "                if st.button(\"Analisar T√≥picos\",\n",
        "                             icon=\":material/hub:\",\n",
        "                             use_container_width=True,\n",
        "                             type=\"primary\",\n",
        "                             help=help_text,\n",
        "                             disabled=disable_topic_button):\n",
        "\n",
        "                    with status_container_topics.status(\"Preparando para an√°lise de t√≥picos...\", expanded=True) as status_topic:\n",
        "                        self.perform_topic_modeling_and_sentiment(status_topic)\n",
        "                    st.session_state['topics_analyzed_toast_shown'] = True\n",
        "                    st.rerun()\n",
        "                # *** FIM DA MODIFICA√á√ÉO ***\n",
        "\n",
        "            with col3_buttons:\n",
        "                if st.button(\"Reiniciar Coleta\", on_click=lambda: st.session_state.update({\n",
        "                    'data': [], 'collection_ended': False, 'collecting': False,\n",
        "                    'sentiment_results': [], 'collected_df': pd.DataFrame(),\n",
        "                    'collected_df_for_download': pd.DataFrame(),\n",
        "                    'stop_event': multiprocessing.Event(), 'data_queue': multiprocessing.Queue(),\n",
        "                    'topic_model_instance': None, 'topic_info_df': pd.DataFrame(),\n",
        "                    'topics_analyzed': False, 'performing_topic_analysis': False,\n",
        "                    'texts_for_topic_analysis': [],\n",
        "                    'sentiment_analysis_toast_shown': False, 'topics_analyzed_toast_shown': False\n",
        "                }), icon=\":material/refresh:\",\n",
        "                                help=\"Reinicie a coleta de dados. Isso apagar√° os dados em mem√≥ria!\", use_container_width=True):\n",
        "                    pass\n",
        "\n",
        "            with col4_buttons:\n",
        "                df_to_download = pd.DataFrame(st.session_state['data']) if st.session_state['data'] else pd.DataFrame()\n",
        "                if not df_to_download.empty:\n",
        "                    st.download_button(\n",
        "                        label=\"Baixar Dados\",\n",
        "                        data=df_to_download.to_json(orient='records', indent=4, date_format='iso'),\n",
        "                        file_name=f'bsky_data_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json',\n",
        "                        mime='application/json',\n",
        "                        help=\"Baixe os dados coletados (incluindo sentimentos e t√≥picos, se analisados) em formato JSON.\",\n",
        "                        icon=\":material/download:\",\n",
        "                        use_container_width=True\n",
        "                    )\n",
        "                else:\n",
        "                    st.button(\"Baixar Dados\", disabled=True, use_container_width=True, help=\"Nenhum dado para baixar.\", icon=\":material/download:\")\n",
        "\n",
        "            if st.session_state.get('topics_analyzed', False) and not st.session_state.get('topic_info_df', pd.DataFrame()).empty:\n",
        "                st.markdown(\"---\")\n",
        "                st.subheader(\"An√°lise de T√≥picos e Sentimentos por T√≥pico\")\n",
        "                st.sidebar.title(\"\")\n",
        "                st.sidebar.info(\n",
        "                    \"**Sobre a An√°lise de T√≥picos:**\\n\\n\"\n",
        "                    \"- T√≥picos s√£o extra√≠dos usando BERTopic.\\n\"\n",
        "                    \"- O t√≥pico '-1' agrupa posts considerados outliers.\\n\"\n",
        "                    \"- 'Palavras-Chave' representam os termos mais significativos para cada t√≥pico.\\n\"\n",
        "                    \"- 'Sentimento por T√≥pico' √© a distribui√ß√£o percentual dos sentimentos dos posts atribu√≠dos a cada t√≥pico (excluindo outliers da agrega√ß√£o).\"\n",
        "                )\n",
        "\n",
        "                source_topic_df = st.session_state['topic_info_df'].copy()\n",
        "\n",
        "                rename_map = {'Topic': 'ID T√≥pico', 'Count': 'N¬∫ Posts', 'Name': 'Palavras-Chave'}\n",
        "                sentiment_cols_original = ['Positive (%)', 'Negative (%)', 'Neutral (%)', 'Error (%)']\n",
        "\n",
        "                display_df = source_topic_df.rename(columns={k: v for k, v in rename_map.items() if k in source_topic_df.columns})\n",
        "\n",
        "                cols_for_display_final = []\n",
        "                for original_name, new_name in rename_map.items():\n",
        "                    if original_name in source_topic_df.columns:\n",
        "                        cols_for_display_final.append(new_name)\n",
        "\n",
        "                for sent_col in sentiment_cols_original:\n",
        "                    if sent_col in display_df.columns:\n",
        "                        cols_for_display_final.append(sent_col)\n",
        "\n",
        "                if 'Palavras-Chave' in display_df.columns and 'Name' in source_topic_df.columns:\n",
        "                    try:\n",
        "                        display_df['Palavras-Chave'] = display_df['Palavras-Chave'].apply(\n",
        "                            lambda x: \", \".join(x.split('_')[1:]) if isinstance(x, str) and '_' in x else x\n",
        "                        )\n",
        "                    except Exception as e:\n",
        "                        st.warning(f\"N√£o foi poss√≠vel formatar a coluna 'Palavras-Chave': {e}\")\n",
        "\n",
        "                cols_for_display_final = [col for col in cols_for_display_final if col in display_df.columns]\n",
        "\n",
        "                if not cols_for_display_final:\n",
        "                    st.warning(\"Nenhuma coluna de informa√ß√£o de t√≥pico para exibir. Mostrando DataFrame de t√≥picos completo (se dispon√≠vel).\", icon=\"‚ö†Ô∏è\")\n",
        "                    if not display_df.empty:\n",
        "                        st.dataframe(display_df, use_container_width=True)\n",
        "                    else:\n",
        "                        st.info(\"N√£o h√° dados de t√≥picos para mostrar.\")\n",
        "                else:\n",
        "                    st.dataframe(display_df[cols_for_display_final], use_container_width=True)\n",
        "\n",
        "                topic_model_instance = st.session_state.get('topic_model_instance')\n",
        "                if topic_model_instance:\n",
        "                    try:\n",
        "                        st.subheader(\"Visualiza√ß√µes dos T√≥picos\")\n",
        "\n",
        "                        num_topics_available = 0\n",
        "                        if not st.session_state.get('topic_info_df', pd.DataFrame()).empty:\n",
        "                            num_topics_available = len(st.session_state['topic_info_df'])\n",
        "\n",
        "                        if num_topics_available > 0:\n",
        "                            st.write(f\"Exibindo visualiza√ß√µes para os {num_topics_available} agrupamentos de t√≥picos identificados.\")\n",
        "\n",
        "                            fig_topics = topic_model_instance.visualize_topics(top_n_topics=num_topics_available)\n",
        "                            st.plotly_chart(fig_topics, use_container_width=True)\n",
        "\n",
        "                            with st.expander(\"üó∫Ô∏è O que esse Gr√°fico mostra?\", expanded=False):\n",
        "                                st.markdown(\"\"\"\n",
        "                                - Pense nele como um mapa onde cada cidade √© um t√≥pico. A posi√ß√£o das \"cidades\" (c√≠rculos) n√£o √© aleat√≥ria; ela representa a similaridade entre os t√≥picos.\n",
        "                                - Cada C√≠rculo √© um T√≥pico: Cada bolha no gr√°fico representa um dos t√≥picos que o modelo encontrou. Ao passar o mouse sobre um c√≠rculo, voc√™ ver√° seu n√∫mero de identifica√ß√£o e as palavras-chave que o definem.\n",
        "                                - O Tamanho dos C√≠rculos: O tamanho de cada c√≠rculo √© proporcional √† frequ√™ncia do t√≥pico, ou seja, ao n√∫mero de posts que foram classificados naquele t√≥pico.\n",
        "                                    - C√≠rculos grandes: T√≥picos muito populares, com muitos posts associados.\n",
        "                                    - C√≠rculos pequenos: T√≥picos de nicho, com menos posts.\n",
        "                                - A Posi√ß√£o e a Dist√¢ncia no Gr√°fico: Esta √© a parte mais importante. Os t√≥picos s√£o plotados de forma que a dist√¢ncia entre eles represente sua similaridade sem√¢ntica.\n",
        "                                    - T√≥picos Pr√≥ximos: T√≥picos que aparecem perto um do outro no mapa s√£o semanticamente semelhantes. Eles usam vocabul√°rio parecido ou discutem assuntos relacionados. Por exemplo, um t√≥pico sobre \"elei√ß√µes\" pode estar perto de um sobre \"economia\".\n",
        "                                    - T√≥picos Distantes: T√≥picos que est√£o longe uns dos outros s√£o semanticamente diferentes. Por exemplo, um t√≥pico sobre \"receitas de bolo\" estaria muito longe de um sobre \"manuten√ß√£o de carros\".\n",
        "                            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "                            barchart_min_height_per_topic = 10\n",
        "                            barchart_base_height = 1\n",
        "                            barchart_height = (num_topics_available * barchart_min_height_per_topic) + barchart_base_height\n",
        "                            if barchart_height < 400:\n",
        "                                barchart_height = 400\n",
        "\n",
        "                            fig_barchart = topic_model_instance.visualize_barchart(\n",
        "                                top_n_topics=num_topics_available,\n",
        "                                height=barchart_height,\n",
        "                                n_words=5\n",
        "                            )\n",
        "                            st.plotly_chart(fig_barchart, use_container_width=True)\n",
        "                            with st.expander(\"üìä O que esse Gr√°fico mostra?\", expanded=False):\n",
        "                                st.markdown(\"\"\"\n",
        "                                - Diferente do mapa anterior que mostrava a rela√ß√£o entre os t√≥picos, este gr√°fico olha para dentro de cada um deles.\n",
        "                                - Cada Sub-gr√°fico √© um T√≥pico: O gr√°fico √© dividido em v√°rios gr√°ficos de barras menores. Cada um desses sub-gr√°ficos corresponde a um √∫nico t√≥pico e √© identificado por seu t√≠tulo (ex: \"Topic 0\", \"Topic 1\", etc.).\n",
        "                                - As Barras e Suas Palavras: Dentro de cada sub-gr√°fico, cada barra representa uma √∫nica palavra. Cada gr√°fico mostra as 5 palavras mais importantes para cada t√≥pico.\n",
        "                                - O Comprimento das Barras (Score c-TF-IDF): Este √© o conceito central. O comprimento de cada barra n√£o √© a contagem da palavra. Ele representa o score c-TF-IDF daquela palavra dentro daquele t√≥pico.\n",
        "                                    - üí° O que √© c-TF-IDF? √â uma m√©trica que o BERTopic usa para medir a import√¢ncia de uma palavra para um t√≥pico espec√≠fico. Uma palavra com um score c-TF-IDF alto √© muito caracter√≠stica daquele t√≥pico e n√£o apenas uma palavra comum em geral. Por exemplo, a palavra \"gato\" pode ter um score alt√≠ssimo no t√≥pico sobre animais de estima√ß√£o, mesmo que a palavra \"disse\" apare√ßa mais vezes em todo o conjunto de dados.\n",
        "                                \"\"\", unsafe_allow_html=True)\n",
        "                        else:\n",
        "                            st.info(\"N√£o h√° t√≥picos suficientes para gerar visualiza√ß√µes gr√°ficas.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.warning(f\"N√£o foi poss√≠vel gerar visualiza√ß√µes dos t√≥picos: {e}\", icon=\"‚ö†Ô∏è\")\n",
        "\n",
        "            if st.session_state.get('topics_analyzed', False) and not df_collected.empty:\n",
        "                st.markdown(\"---\")\n",
        "                st.subheader(\"Dados Coletados Detalhados (com ID do T√≥pico)\")\n",
        "                cols_to_show_detailed = df_collected.columns.tolist()\n",
        "                st.dataframe(df_collected[cols_to_show_detailed], use_container_width=True)\n",
        "\n",
        "\n",
        "        elif st.session_state['collection_ended'] and not st.session_state['data']:\n",
        "            st.warning(\"Nenhum post foi coletado durante o per√≠odo especificado ou que corresponda aos crit√©rios.\", icon=\"‚ö†Ô∏è\")\n",
        "            if st.button(\"Tentar Nova Coleta\", icon=\":material/refresh:\", use_container_width=True):\n",
        "                st.session_state.update({\n",
        "                    'data': [], 'collection_ended': False, 'collecting': False,\n",
        "                    'sentiment_results': [], 'collected_df': pd.DataFrame(),\n",
        "                    'collected_df_for_download': pd.DataFrame(),\n",
        "                    'stop_event': multiprocessing.Event(), 'data_queue': multiprocessing.Queue(),\n",
        "                    'topic_model_instance': None, 'topic_info_df': pd.DataFrame(),\n",
        "                    'topics_analyzed': False, 'performing_topic_analysis': False,\n",
        "                    'texts_for_topic_analysis': [],\n",
        "                    'sentiment_analysis_toast_shown': False, 'topics_analyzed_toast_shown': False\n",
        "                })\n",
        "                st.rerun()\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def run(self):\n",
        "        st.markdown(f\"<div style='text-align: left;'>{self.bskylogo_svg_template}</div>\", unsafe_allow_html=True)\n",
        "        st.text(\"\")\n",
        "        st.text(\"\")\n",
        "\n",
        "        st.sidebar.markdown(self.bskylogo_svg_template, unsafe_allow_html=True)\n",
        "        st.sidebar.title(\"BskyMood\")\n",
        "        st.sidebar.markdown(\"**Coleta e An√°lise de Sentimentos em Tempo Real no Bluesky**\")\n",
        "\n",
        "        if not st.session_state['collecting'] and not st.session_state['collection_ended']:\n",
        "            st.warning(\n",
        "                \"Nenhum post coletado ainda. Clique no bot√£o 'Iniciar Coleta' para come√ßar.\",\n",
        "                icon=\":material/warning:\"\n",
        "            )\n",
        "            st.sidebar.info(\n",
        "                \"**Antes de come√ßar**\\n\\n\"\n",
        "                \"- Selecione um intervalo de coleta e clique em 'Iniciar Coleta'.\\n\"\n",
        "                \"- Intervalos maiores implicam em maior tempo de coleta e processamento.\\n\"\n",
        "                \"- As postagens podem incluir termos ofensivos ou inadequados.\"\n",
        "            )\n",
        "\n",
        "            st.session_state['collection_duration'] = st.sidebar.slider(\n",
        "                \"Dura√ß√£o da Coleta (segundos)\", min_value=10, max_value=300, value=30, step=5,\n",
        "                help=\"Defina por quanto tempo os posts ser√£o coletados.\"\n",
        "            )\n",
        "\n",
        "            if st.sidebar.button(\"Iniciar Coleta\", icon=\":material/play_circle:\", use_container_width=True, type=\"primary\"):\n",
        "                st.session_state.update({\n",
        "                    'data': [], 'collection_ended': False, 'collecting': True,\n",
        "                    'sentiment_results': [], 'collected_df': pd.DataFrame(),\n",
        "                    'collected_df_for_download': pd.DataFrame(),\n",
        "                    'stop_event': multiprocessing.Event(), 'data_queue': multiprocessing.Queue(),\n",
        "                    'topic_model_instance': None, 'topic_info_df': pd.DataFrame(),\n",
        "                    'topics_analyzed': False, 'performing_topic_analysis': False,\n",
        "                    'texts_for_topic_analysis': [],\n",
        "                    'sentiment_analysis_toast_shown': False, 'topics_analyzed_toast_shown': False\n",
        "                })\n",
        "                st.session_state['stop_event'].clear()\n",
        "                st.rerun()\n",
        "\n",
        "        elif st.session_state['collecting']:\n",
        "            self.collect_data()\n",
        "\n",
        "        if not st.session_state['collecting']:\n",
        "            self.display_data()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = BskyDataCollectorApp()\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "ehf_gvp8UhhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Aten√ß√£o**\n",
        "A c√©lula 5 inicia a execu√ß√£o do aplicativo em uma nova aba do seu navegador. Na p√°gina, insira a senha copiada anteriormente e clique no bot√£o azul para confirmar.\n",
        "\n",
        "O BskyMood ser√° carregado. Aguarde alguns minutos para sua estabiliza√ß√£o antes de come√ßar a us√°-lo."
      ],
      "metadata": {
        "id": "3Rj3GSLHWxKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Exposi√ß√£o do localtunnel e Execu√ß√£o do BskyMood\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "VXABiDfHU2WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Para encerrar a execu√ß√£o, interrompa a c√©lula do Passo 5.**"
      ],
      "metadata": {
        "id": "slbw-i31VBJX"
      }
    }
  ]
}