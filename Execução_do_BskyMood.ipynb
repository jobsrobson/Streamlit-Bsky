{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobsrobson/Streamlit-Bsky/blob/main/Execu%C3%A7%C3%A3o_do_BskyMood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIocBIkbRevV"
      },
      "source": [
        "# üöÄ Execu√ß√£o do BskyMood via Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbqmOON9Rruw"
      },
      "source": [
        "#### Vis√£o Geral\n",
        "Este notebook serve como ambiente de execu√ß√£o para o aplicativo **BskyMood**, uma ferramenta de coleta e an√°lise de sentimentos e t√≥picos da rede social Bluesky, constru√≠da com Streamlit.\n",
        "\n",
        "Devido √†s exig√™ncias computacionais dos modelos de Machine Learning utilizados, a execu√ß√£o do aplicativo na nuvem gratuita do Streamlit (Streamlit Community Cloud) torna-se invi√°vel. Adotei, portanto, uma abordagem alternativa que combina o poder computacional do Google Colab com a praticidade do `localtunnel` para expor o aplicativo na internet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzjhDgi1SLeM"
      },
      "source": [
        "#### Motiva√ß√£o\n",
        "\n",
        "A plataforma Streamlit Community Cloud √© fant√°stica para hospedar a maioria dos aplicativos, mas possui limita√ß√µes de recursos no seu plano gratuito (tipicamente 1 GB de RAM e poder de CPU restrito). O aplicativo ultrapassa esses limites por duas raz√µes principais:\n",
        "\n",
        "1. **An√°lise de Sentimentos com Transformers:** Carregamos um modelo de linguagem da Hugging Face (lxyuan/distilbert-base-multilingual-cased-sentiments-student) para realizar a an√°lise de sentimentos. Esses modelos, mesmo os \"distilados\", consomem uma quantidade significativa de mem√≥ria RAM para serem carregados e processados.\n",
        "\n",
        "2. **Modelagem de T√≥picos com BERTopic:** A biblioteca BERTopic √© computacionalmente intensiva. O processo envolve a cria√ß√£o de embeddings de alta dimensionalidade para todos os posts, a redu√ß√£o dessa dimensionalidade (usando UMAP) e a clusteriza√ß√£o (usando HDBSCAN). Essas etapas demandam um uso consider√°vel de CPU e RAM, especialmente √† medida que o n√∫mero de posts coletados aumenta.\n",
        "\n",
        "Solu√ß√£o: O Google Colab oferece um ambiente com recursos muito mais robustos (geralmente mais de 12 GB de RAM e acesso a GPUs), permitindo que o aplicativo execute essas tarefas pesadas sem falhar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChLNYtNES6J1"
      },
      "source": [
        "#### Como Funciona?\n",
        "\n",
        "Ao executar um aplicativo Streamlit em um notebook do Colab, ele roda em um servidor web local dentro da m√°quina virtual do Colab. Por padr√£o, esse servidor n√£o √© acess√≠vel pela internet p√∫blica. √â aqui que o `localtunnel` entra em a√ß√£o.\n",
        "\n",
        "`localtunnel` √© uma ferramenta que exp√µe seu servidor web local √† internet de forma segura. Ele funciona da seguinte maneira:\n",
        "\n",
        "- O aplicativo Streamlit √© iniciado e fica \"escutando\" em uma porta local (ex: porta 8501) dentro do ambiente do Colab.\n",
        "- Executamos o localtunnel, apontando para essa porta.\n",
        "O localtunnel cria uma URL p√∫blica √∫nica (ex: https://seu-nome-aleatorio.loca.lt).\n",
        "- Qualquer pessoa que acesse essa URL p√∫blica ter√° sua requisi√ß√£o \"tunelada\" de forma segura diretamente para o nosso aplicativo Streamlit rodando no Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ll63M7HTLa4"
      },
      "source": [
        "### **Execute todas as c√©lulas abaixo, em ordem.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWvFf6nVRefa"
      },
      "outputs": [],
      "source": [
        "# 1. Instala√ß√£o das Bibliotecas Necess√°rias (Python)\n",
        "!pip install streamlit pandas atproto regex langdetect transformers torch emoji bertopic scikit-learn nltk -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjoaJdNsQ-aH"
      },
      "outputs": [],
      "source": [
        "# 2. Instala√ß√£o do localtunnel (NPM)\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIcuolOhUHKy"
      },
      "outputs": [],
      "source": [
        "# 3. Obten√ß√£o da senha de acesso ao localtunnel exposto\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "# COPIE E COLE ESTA SENHA QUANDO FOR SOLICITADO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehf_gvp8UhhV"
      },
      "outputs": [],
      "source": [
        "# 4. Grava√ß√£o do BskyMood em um arquivo .py dentro do armazenamento do Colab\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from atproto import FirehoseSubscribeReposClient, parse_subscribe_repos_message, CAR, IdResolver, DidInMemoryCache\n",
        "import time\n",
        "import multiprocessing\n",
        "import threading\n",
        "import regex as re\n",
        "from langdetect import detect\n",
        "import queue\n",
        "from datetime import datetime\n",
        "from transformers import pipeline\n",
        "import emoji\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "\n",
        "# Download das Stopwords do NLTK. Este bloco √© executado uma vez no in√≠cio da aplica√ß√£o.\n",
        "try:\n",
        "  nltk.download('stopwords', quiet=True)\n",
        "except Exception as e:\n",
        "  # N√£o bloquear a UI se o download falhar. Um aviso √© mostrado.\n",
        "  st.toast(f\"Alerta: N√£o foi poss√≠vel baixar stopwords do NLTK: {e}. A modelagem de t√≥picos prosseguir√° com as configura√ß√µes padr√£o.\", icon=\"‚ö†Ô∏è\")\n",
        "  print(f\"Alerta: N√£o foi poss√≠vel baixar stopwords do NLTK: {e}.\")\n",
        "\n",
        "\n",
        "class BskyDataCollectorApp:\n",
        "    \"\"\"\n",
        "    Classe principal que encapsula toda a l√≥gica do aplicativo BskyMood.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Construtor da classe. Configura a p√°gina do Streamlit, inicializa\n",
        "        vari√°veis e os estados da sess√£o.\n",
        "        \"\"\"\n",
        "        st.set_page_config(page_title='BskyMood', layout='wide')\n",
        "        self.svg_path_data = \"\"\"\n",
        "            M13.873 3.805C21.21 9.332 29.103 20.537 32 26.55v15.882c0-.338-.13.044-.41.867-1.512 4.456-7.418 21.847-20.923 7.944-7.111-7.32-3.819-14.64 9.125-16.85-7.405 1.264-15.73-.825-18.014-9.015C1.12 23.022 0 8.51 0 6.55 0-3.268 8.579-.182 13.873 3.805ZM50.127 3.805C42.79 9.332 34.897 20.537 32 26.55v15.882c0-.338.13.044.41.867 1.512 4.456 7.418 21.847 20.923 7.944 7.111-7.32 3.819-14.64-9.125-16.85 7.405 1.264 15.73-.825 18.014-9.015C62.88 23.022 64 8.51 64 6.55c0-9.818-8.578-6.732-13.873-2.745Z\n",
        "        \"\"\"\n",
        "        self.bskylogo_svg_template = f\"\"\"\n",
        "            <svg width=\"28\" height=\"28\" viewBox=\"0 0 64 64\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "                <path d=\"{self.svg_path_data}\" fill=\"#0085ff\"/>\n",
        "            </svg>\n",
        "        \"\"\"\n",
        "        self._initialize_session_state()\n",
        "        self._initialize_topic_session_state()\n",
        "        self.resolver_cache = DidInMemoryCache()\n",
        "        self.sentiment_pipeline = None\n",
        "        self.topic_model = None\n",
        "\n",
        "\n",
        "    def _initialize_session_state(self):\n",
        "        \"\"\"\n",
        "        Inicializa os estados da sess√£o do Streamlit para dados gerais e de coleta.\n",
        "        Garante que as vari√°veis persistam entre as intera√ß√µes do usu√°rio.\n",
        "        \"\"\"\n",
        "        if 'data' not in st.session_state:\n",
        "            st.session_state['data'] = []\n",
        "        if 'collecting' not in st.session_state:\n",
        "            st.session_state['collecting'] = False\n",
        "        if 'collection_ended' not in st.session_state:\n",
        "            st.session_state['collection_ended'] = False\n",
        "        if 'stop_event' not in st.session_state:\n",
        "            st.session_state['stop_event'] = multiprocessing.Event()\n",
        "        if 'start_time' not in st.session_state:\n",
        "            st.session_state['start_time'] = 0.0\n",
        "        if 'data_queue' not in st.session_state:\n",
        "            st.session_state['data_queue'] = multiprocessing.Queue()\n",
        "        if 'sentiment_results' not in st.session_state:\n",
        "            st.session_state['sentiment_results'] = []\n",
        "        if 'collected_df' not in st.session_state:\n",
        "            st.session_state['collected_df'] = pd.DataFrame()\n",
        "\n",
        "\n",
        "    def _initialize_topic_session_state(self):\n",
        "        \"\"\"\n",
        "        Inicializa os estados da sess√£o espec√≠ficos para a an√°lise de t√≥picos.\n",
        "        \"\"\"\n",
        "        if 'topic_model_instance' not in st.session_state:\n",
        "            st.session_state['topic_model_instance'] = None\n",
        "        if 'topic_info_df' not in st.session_state:\n",
        "            st.session_state['topic_info_df'] = pd.DataFrame()\n",
        "        if 'topics_analyzed' not in st.session_state:\n",
        "            st.session_state['topics_analyzed'] = False\n",
        "        if 'performing_topic_analysis' not in st.session_state:\n",
        "            st.session_state['performing_topic_analysis'] = False\n",
        "        if 'texts_for_topic_analysis' not in st.session_state:\n",
        "            st.session_state['texts_for_topic_analysis'] = []\n",
        "\n",
        "\n",
        "    def _process_message(self, message, data_queue):\n",
        "        \"\"\"\n",
        "        Processa uma √∫nica mensagem recebida do Firehose.\n",
        "        Filtra por posts criados e os coloca na fila de dados.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            commit = parse_subscribe_repos_message(message)\n",
        "            if not hasattr(commit, 'ops'):\n",
        "                return\n",
        "\n",
        "            for op in commit.ops:\n",
        "                if op.action == 'create' and op.path.startswith('app.bsky.feed.post/'):\n",
        "                    post_data = self._extract_post_data(commit, op)\n",
        "                    if post_data and self._lang_selector(post_data['text']):\n",
        "                        data_queue.put(post_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing message in thread: {e}\")\n",
        "\n",
        "\n",
        "    def _lang_selector(self, text):\n",
        "        \"\"\"\n",
        "        Detecta o idioma do texto e retorna True se for ingl√™s, portugu√™s ou espanhol.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            lang = detect(text)\n",
        "            return lang in ['en', 'pt', 'es']\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _extract_post_data(self, commit, op):\n",
        "        \"\"\"\n",
        "        Extrai os dados relevantes de um post (skeet) a partir do objeto CAR.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            car = CAR.from_bytes(commit.blocks)\n",
        "            author_handle = commit.repo\n",
        "\n",
        "            for record in car.blocks.values():\n",
        "                if isinstance(record, dict) and record.get('$type') == 'app.bsky.feed.post':\n",
        "                    return {\n",
        "                        'text': record.get('text', ''),\n",
        "                        'created_at': record.get('createdAt', ''),\n",
        "                        'author': author_handle,\n",
        "                        'uri': f'at://{commit.repo}/{op.path}',\n",
        "                        'has_images': 'embed' in record,\n",
        "                        'reply_to': record.get('reply', {}).get('parent', {}).get('uri')\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            st.toast(f\"Erro ao extrair dados: {e}\", icon=\":material/dangerous:\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def _collect_messages_threaded(self, stop_event, data_queue):\n",
        "        \"\"\"\n",
        "        Inicia a coleta de mensagens em uma thread separada para n√£o bloquear a UI.\n",
        "        \"\"\"\n",
        "        client = FirehoseSubscribeReposClient()\n",
        "        try:\n",
        "            client.start(lambda message: self._process_message(message, data_queue))\n",
        "        except Exception as e:\n",
        "            st.toast(f\"Erro na thread de coleta: {e}\", icon=\":material/dangerous:\")\n",
        "        finally:\n",
        "            client.stop()\n",
        "\n",
        "\n",
        "    def collect_data(self):\n",
        "        \"\"\"\n",
        "        Gerencia o processo de coleta de dados, incluindo a UI (bot√£o de parar, status).\n",
        "        \"\"\"\n",
        "        stop_event = st.session_state['stop_event']\n",
        "        data_queue = st.session_state['data_queue']\n",
        "        st.session_state['collection_ended'] = False\n",
        "\n",
        "        if st.session_state['collecting'] and not st.session_state['collection_ended']:\n",
        "            stop_button_pressed = st.button(\"Parar Coleta\", icon=\":material/stop_circle:\", help=\"Clique para parar a coleta de dados. Os dados j√° coletados ser√£o mantidos na mem√≥ria.\")\n",
        "            if stop_button_pressed:\n",
        "                st.session_state['stop_event'].set()\n",
        "                st.session_state['collecting'] = False\n",
        "\n",
        "        start_time = time.time()\n",
        "        collection_duration = st.session_state.get('collection_duration', 30)\n",
        "        collecting_data_flag = st.session_state['collecting']\n",
        "\n",
        "        if collecting_data_flag:\n",
        "            collection_thread = threading.Thread(target=self._collect_messages_threaded, args=(stop_event, data_queue))\n",
        "            collection_thread.daemon = True\n",
        "            collection_thread.start()\n",
        "\n",
        "            with st.status(f\"Coletando posts do Bluesky durante {collection_duration} segundos. Aguarde!\") as status:\n",
        "                mensagens = [\n",
        "                    \"Estabelecendo conex√£o com o Firehose...\", \"Conex√£o estabelecida com sucesso!\",\n",
        "                    \"Autenticando...\", \"Autentica√ß√£o conclu√≠da!\", \"Organizando a fila...\",\n",
        "                    \"Atualizando lista...\", \"Coletando posts... Isso pode demorar alguns minutos.\",\n",
        "                ]\n",
        "                for i, msg in enumerate(mensagens):\n",
        "                    status.update(label=msg)\n",
        "                    if i < len(mensagens) - 1:\n",
        "                        time.sleep(1 if i != 0 else 2)\n",
        "                        if stop_event.is_set() or (time.time() - start_time >= collection_duration):\n",
        "                            break\n",
        "                    else:\n",
        "                        while collecting_data_flag and not stop_event.is_set() and (time.time() - start_time < collection_duration):\n",
        "                            try:\n",
        "                                while not data_queue.empty():\n",
        "                                    st.session_state['data'].append(data_queue.get_nowait())\n",
        "                            except queue.Empty:\n",
        "                                pass\n",
        "                            except Exception as e:\n",
        "                                print(f\"Erro ao recuperar da fila: {e}\")\n",
        "                            time.sleep(0.5)\n",
        "                            collecting_data_flag = st.session_state['collecting']\n",
        "                            if not collecting_data_flag:\n",
        "                                stop_event.set()\n",
        "                try:\n",
        "                    while not data_queue.empty():\n",
        "                        st.session_state['data'].append(data_queue.get_nowait())\n",
        "                except queue.Empty:\n",
        "                    pass\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao recuperar dados restantes da fila: {e}\")\n",
        "\n",
        "            st.session_state['collecting'] = False\n",
        "            st.session_state['collection_ended'] = True\n",
        "            stop_event.set()\n",
        "            if collection_thread.is_alive():\n",
        "                collection_thread.join(timeout=2)\n",
        "            st.rerun()\n",
        "\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Limpa e pr√©-processa o texto de uma publica√ß√£o.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return ''\n",
        "\n",
        "        text = re.sub(r'@\\S+', '', text)\n",
        "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "        text = re.sub(r'www\\.\\S+', '', text)\n",
        "        text = re.sub(r'\\b[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}\\b(/\\S*)?', '', text, flags=re.IGNORECASE)\n",
        "        text = emoji.demojize(text, language='en')\n",
        "        return text\n",
        "\n",
        "    def analyze_sentiment(self, status_obj):\n",
        "        \"\"\"\n",
        "        Executa a an√°lise de sentimentos nos dados coletados.\n",
        "        \"\"\"\n",
        "        if not self.sentiment_pipeline:\n",
        "            try:\n",
        "                status_obj.update(label=\"Carregando modelo de an√°lise de sentimentos...\")\n",
        "                self.sentiment_pipeline = pipeline(\n",
        "                    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
        "                    return_all_scores=False,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                st.error(f\"Erro ao carregar o modelo de an√°lise de sentimentos: {e}\", icon=\":material/error:\")\n",
        "                status_obj.update(label=\"Falha ao carregar modelo de an√°lise.\", state=\"error\", expanded=True)\n",
        "                return\n",
        "\n",
        "        st.session_state['sentiment_results'] = []\n",
        "        updated_data_with_sentiment = []\n",
        "        if st.session_state['collection_ended'] and st.session_state['data']:\n",
        "            total_posts = len(st.session_state['data'])\n",
        "            for i, post in enumerate(st.session_state['data']):\n",
        "                status_obj.update(label=f\"Analisando post {i+1}/{total_posts}: \\\"{post['text'][:50]}...\\\"\")\n",
        "                try:\n",
        "                    processed_text = self.preprocess_text(post['text'])\n",
        "                    sentiment = \"neutral\" if not processed_text.strip() else self.sentiment_pipeline(processed_text)[0]['label']\n",
        "                    post_with_sentiment = post.copy()\n",
        "                    post_with_sentiment['sentiment'] = sentiment\n",
        "                    updated_data_with_sentiment.append(post_with_sentiment)\n",
        "                    st.session_state['sentiment_results'].append({'text': post['text'], 'sentiment': sentiment})\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Erro ao analisar o sentimento do post \\\"{post['text'][:50]}...\\\": {e}\", icon=\":material/error:\")\n",
        "                    post_with_error = post.copy()\n",
        "                    post_with_error['sentiment'] = 'analysis_error'\n",
        "                    updated_data_with_sentiment.append(post_with_error)\n",
        "\n",
        "            status_obj.update(label=\"An√°lise de sentimentos conclu√≠da!\", state=\"complete\", expanded=False)\n",
        "            st.session_state['data'] = updated_data_with_sentiment\n",
        "        else:\n",
        "            st.error(\"N√£o h√° dados coletados para an√°lise de sentimentos.\", icon=\":material/error:\")\n",
        "            status_obj.update(label=\"Nenhum dado para analisar.\", state=\"error\", expanded=True)\n",
        "\n",
        "\n",
        "    def perform_topic_modeling_and_sentiment(self, status_obj):\n",
        "        \"\"\"\n",
        "        Executa a modelagem de t√≥picos e a an√°lise de sentimentos agregada por t√≥pico.\n",
        "        \"\"\"\n",
        "        st.session_state['performing_topic_analysis'] = True\n",
        "        st.session_state['topics_analyzed'] = False\n",
        "\n",
        "        if not st.session_state.get('data'):\n",
        "            st.warning(\"N√£o h√° dados coletados para a an√°lise de t√≥picos.\", icon=\"‚ö†Ô∏è\")\n",
        "            status_obj.update(label=\"Nenhum dado para an√°lise.\", state=\"error\", expanded=True)\n",
        "            st.session_state['performing_topic_analysis'] = False\n",
        "            return\n",
        "\n",
        "        status_obj.update(label=\"Preparando textos para modelagem...\")\n",
        "        texts_for_bertopic = [self.preprocess_text(post.get('text', '')) for post in st.session_state['data']]\n",
        "        st.session_state['texts_for_topic_analysis'] = texts_for_bertopic\n",
        "\n",
        "        if not any(texts_for_bertopic):\n",
        "            st.warning(\"Nenhum texto v√°lido encontrado nos posts para a an√°lise de t√≥picos.\", icon=\"‚ö†Ô∏è\")\n",
        "            status_obj.update(label=\"Nenhum texto para modelagem.\", state=\"error\", expanded=True)\n",
        "            st.session_state['performing_topic_analysis'] = False\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            all_stop_words = list(nltk.corpus.stopwords.words('english')) + list(nltk.corpus.stopwords.words('portuguese')) + list(nltk.corpus.stopwords.words('spanish'))\n",
        "            vectorizer_model = CountVectorizer(stop_words=all_stop_words)\n",
        "        except Exception as e:\n",
        "            st.warning(f\"N√£o foi poss√≠vel carregar stopwords: {e}. Usando BERTopic com configura√ß√µes padr√£o.\", icon=\"‚ö†Ô∏è\")\n",
        "            vectorizer_model = None\n",
        "\n",
        "        try:\n",
        "            status_obj.update(label=\"Iniciando modelagem de t√≥picos com BERTopic... Isso pode levar alguns minutos.\")\n",
        "            self.topic_model = BERTopic(language=\"multilingual\",\n",
        "                                        vectorizer_model=vectorizer_model, \n",
        "                                        min_topic_size=3, \n",
        "                                        verbose=True)\n",
        "\n",
        "            topics, _ = self.topic_model.fit_transform(texts_for_bertopic)\n",
        "            st.session_state['topic_model_instance'] = self.topic_model\n",
        "\n",
        "            status_obj.update(label=\"Modelagem conclu√≠da. Processando resultados...\")\n",
        "\n",
        "            if len(st.session_state['data']) == len(topics):\n",
        "                for i, post_data in enumerate(st.session_state['data']):\n",
        "                    post_data['topic_id'] = topics[i]\n",
        "            \n",
        "            topic_info_df = self.topic_model.get_topic_info()\n",
        "            posts_df_for_topic_sentiment = pd.DataFrame(st.session_state['data'])\n",
        "\n",
        "            if 'sentiment' in posts_df_for_topic_sentiment.columns and 'topic_id' in posts_df_for_topic_sentiment.columns:\n",
        "                status_obj.update(label=\"Analisando sentimentos por t√≥pico...\")\n",
        "                sentiment_by_topic = posts_df_for_topic_sentiment[posts_df_for_topic_sentiment['topic_id'] != -1] \\\n",
        "                                     .groupby('topic_id')['sentiment'].value_counts(normalize=True).unstack(fill_value=0)\n",
        "                \n",
        "                sentiment_by_topic = sentiment_by_topic.rename(columns=lambda x: f\"{x.capitalize()} (%)\" if x != 'analysis_error' else 'Error (%)')\n",
        "                \n",
        "                for col in sentiment_by_topic.columns:\n",
        "                    sentiment_by_topic[col] = (sentiment_by_topic[col] * 100).round(1)\n",
        "\n",
        "                if 'Topic' in topic_info_df.columns:\n",
        "                    topic_info_df = topic_info_df.merge(sentiment_by_topic, left_on='Topic', right_index=True, how='left').fillna(0)\n",
        "            \n",
        "            st.session_state['topic_info_df'] = topic_info_df\n",
        "            st.session_state['topics_analyzed'] = True\n",
        "            status_obj.update(label=\"An√°lise de t√≥picos e sentimentos conclu√≠da!\", state=\"complete\", expanded=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erro durante a modelagem de t√≥picos: {e}\", icon=\":material/error:\")\n",
        "            status_obj.update(label=f\"Erro na modelagem de t√≥picos: {e}\", state=\"error\", expanded=True)\n",
        "        finally:\n",
        "            st.session_state['performing_topic_analysis'] = False\n",
        "\n",
        "\n",
        "    def display_data(self):\n",
        "        \"\"\"\n",
        "        Renderiza a interface principal, exibindo dados, m√©tricas, bot√µes e resultados das an√°lises.\n",
        "        \"\"\"\n",
        "        if len(st.session_state['data']) > 0:\n",
        "            df_collected = pd.DataFrame(st.session_state['data'])\n",
        "            st.session_state['collected_df'] = df_collected\n",
        "            num_rows = len(df_collected)\n",
        "\n",
        "            num_has_images = df_collected['has_images'].sum() if 'has_images' in df_collected.columns else 0\n",
        "            num_is_reply = df_collected['reply_to'].notna().sum() if 'reply_to' in df_collected.columns else 0\n",
        "\n",
        "            if st.session_state['collection_ended'] and not st.session_state.get('performing_topic_analysis', False) and not st.session_state.get('collecting', False):\n",
        "                if not st.session_state.get('topics_analyzed_toast_shown', False) and not st.session_state.get('sentiment_analysis_toast_shown', False):\n",
        "                    st.toast(f\"A√ß√£o finalizada com sucesso!\", icon=\":material/check_circle:\")\n",
        "\n",
        "            if 'sentiment' in df_collected.columns and st.session_state.get('sentiment_results') and not st.session_state.get('topics_analyzed'):\n",
        "                total_analyzed = len(df_collected)\n",
        "                sentiment_counts = df_collected['sentiment'].value_counts()\n",
        "                positive_count = sentiment_counts.get('positive', 0)\n",
        "                negative_count = sentiment_counts.get('negative', 0)\n",
        "                neutral_count = sentiment_counts.get('neutral', 0)\n",
        "                positive_percentage = (positive_count / total_analyzed) * 100 if total_analyzed > 0 else 0\n",
        "                negative_percentage = (negative_count / total_analyzed) * 100 if total_analyzed > 0 else 0\n",
        "                neutral_percentage = (neutral_count / total_analyzed) * 100 if total_analyzed > 0 else 0\n",
        "                st.subheader(\"Resultados da An√°lise de Sentimentos\")\n",
        "                col_metric1, col_metric2, col_metric3, col_metric4 = st.columns(4, gap=\"small\", border=True)\n",
        "                with col_metric1: st.metric(label=\"Total de Posts Analisados\", value=total_analyzed)\n",
        "                with col_metric2: st.metric(label=\"Posts Positivos\", value=f\"{positive_percentage:.1f}%\")\n",
        "                with col_metric3: st.metric(label=\"Posts Negativos\", value=f\"{negative_percentage:.1f}%\")\n",
        "                with col_metric4: st.metric(label=\"Posts Neutros\", value=f\"{neutral_percentage:.1f}%\")\n",
        "            \n",
        "            elif not st.session_state.get('topics_analyzed'):\n",
        "                col1_metrics, col2_metrics, col3_metrics = st.columns(3, gap=\"small\", border=True)\n",
        "                with col1_metrics: st.metric(label=\"Total de Posts Coletados\", value=num_rows)\n",
        "                with col2_metrics: st.metric(label=\"Posts com Imagens\", value=num_has_images)\n",
        "                with col3_metrics: st.metric(label=\"Posts em Reply\", value=num_is_reply)\n",
        "\n",
        "            st.session_state['collected_df_for_download'] = df_collected\n",
        "\n",
        "            if 'sentiment' in df_collected.columns and st.session_state.get('sentiment_results') and not st.session_state.get('topics_analyzed', False):\n",
        "                \n",
        "                st.sidebar.warning(\n",
        "                    \"- A an√°lise de sentimentos √© realizada automaticamente e pode n√£o refletir a inten√ß√£o original do autor.\\n\"\n",
        "                    \"- Men√ß√µes e URLs s√£o removidos durante a an√°lise, mas s√£o exibidos na tabela para registro.\"\n",
        "                )\n",
        "                columns_to_show = ['text', 'sentiment']\n",
        "                st.dataframe(df_collected[columns_to_show], use_container_width=True)\n",
        "            \n",
        "            elif not df_collected.empty and not st.session_state.get('topics_analyzed', False):\n",
        "                st.subheader(\"Dados Coletados\")\n",
        "                st.sidebar.warning(\n",
        "                    \"- Para executar a an√°lise de sentimentos, clique em 'Analisar Sentimentos'.\\n\"\n",
        "                    \"- Para executar a an√°lise de t√≥picos, conclua a an√°lise de sentimentos primeiro.\\n\"\n",
        "                    \"- **Aten√ß√£o**: as an√°lises podem levar v√°rios minutos, dependendo da sua conex√£o e do n√∫mero de posts coletados.\"\n",
        "                )\n",
        "                cols_to_display = ['text', 'created_at', 'author', 'has_images', 'reply_to']\n",
        "                st.dataframe(df_collected[cols_to_display], use_container_width=True)\n",
        "\n",
        "            # Exibir bot√µes de a√ß√£o\n",
        "            col1_buttons, col2_buttons, col3_buttons, col4_buttons = st.columns([1.7, 1.7, 1, 1])\n",
        "            status_container_sentiment = st.empty()\n",
        "            status_container_topics = st.empty()\n",
        "\n",
        "            with col1_buttons:\n",
        "                if not st.session_state.get('sentiment_results') and 'sentiment' not in df_collected.columns:\n",
        "                    if st.button(\"Analisar Sentimentos\", icon=\":material/psychology:\", use_container_width=True, type=\"primary\", help=\"Clique para analisar os sentimentos dos posts coletados individualmente.\"):\n",
        "                        with status_container_sentiment.status(\"Preparando para an√°lise de sentimentos...\", expanded=True) as status:\n",
        "                            self.analyze_sentiment(status)\n",
        "                        st.session_state['sentiment_analysis_toast_shown'] = True\n",
        "                        st.rerun()\n",
        "                elif 'sentiment' in df_collected.columns:\n",
        "                    st.button(\"Analisar Sentimentos\", icon=\":material/psychology:\", use_container_width=True, type=\"primary\", help=\"Sentimentos individuais j√° analisados.\", disabled=True)\n",
        "\n",
        "            with col2_buttons:\n",
        "                sentiment_analysis_done = 'sentiment' in df_collected.columns\n",
        "                topics_already_analyzed = st.session_state.get('topics_analyzed', False)\n",
        "                disable_topic_button = topics_already_analyzed or not sentiment_analysis_done\n",
        "\n",
        "                if topics_already_analyzed: help_text = \"T√≥picos j√° analisados.\"\n",
        "                elif not sentiment_analysis_done: help_text = \"Execute a 'An√°lise de Sentimentos' primeiro.\"\n",
        "                else: help_text = \"Clique para extrair t√≥picos e analisar sentimentos por t√≥pico.\"\n",
        "\n",
        "                if st.button(\"Analisar T√≥picos\", icon=\":material/hub:\", use_container_width=True, type=\"primary\", help=help_text, disabled=disable_topic_button):\n",
        "                    with status_container_topics.status(\"Preparando para an√°lise de t√≥picos...\", expanded=True) as status_topic:\n",
        "                        self.perform_topic_modeling_and_sentiment(status_topic)\n",
        "                    st.session_state['topics_analyzed_toast_shown'] = True\n",
        "                    st.rerun()\n",
        "\n",
        "            with col3_buttons:\n",
        "                if st.button(\"Reiniciar Coleta\", on_click=lambda: st.session_state.update({\n",
        "                    'data': [], 'collection_ended': False, 'collecting': False, 'sentiment_results': [], \n",
        "                    'collected_df': pd.DataFrame(), 'collected_df_for_download': pd.DataFrame(),\n",
        "                    'stop_event': multiprocessing.Event(), 'data_queue': multiprocessing.Queue(),\n",
        "                    'topic_model_instance': None, 'topic_info_df': pd.DataFrame(), 'topics_analyzed': False, \n",
        "                    'performing_topic_analysis': False, 'texts_for_topic_analysis': [],\n",
        "                    'sentiment_analysis_toast_shown': False, 'topics_analyzed_toast_shown': False\n",
        "                }), icon=\":material/refresh:\", help=\"Reinicie a coleta. Isso apagar√° todos os dados!\", use_container_width=True):\n",
        "                    pass\n",
        "\n",
        "            with col4_buttons:\n",
        "                df_to_download = pd.DataFrame(st.session_state['data']) if st.session_state['data'] else pd.DataFrame()\n",
        "                if not df_to_download.empty:\n",
        "                    st.download_button(\n",
        "                        label=\"Baixar Dados\", data=df_to_download.to_json(orient='records', indent=4, date_format='iso'),\n",
        "                        file_name=f'bsky_data_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json', mime='application/json',\n",
        "                        help=\"Baixe os dados coletados (incluindo sentimentos e t√≥picos) em formato JSON.\",\n",
        "                        icon=\":material/download:\", use_container_width=True\n",
        "                    )\n",
        "                else:\n",
        "                    st.button(\"Baixar Dados\", disabled=True, use_container_width=True, help=\"Nenhum dado para baixar.\", icon=\":material/download:\")\n",
        "            \n",
        "            if st.session_state.get('topics_analyzed', False) and not st.session_state.get('topic_info_df', pd.DataFrame()).empty:\n",
        "                with st.container(border=True):\n",
        "                    tab1, tab2, tab3, tab4, tab5 = st.tabs([\"üìä Sentimentos por T√≥picos\", \"üó∫Ô∏è Mapa de T√≥picos\", \"üóùÔ∏è Palavras-Chave\", \"üîç Pesquisa de T√≥picos\", \"üìã Dados Coletados\"])\n",
        "\n",
        "                    with tab1:\n",
        "                        st.subheader(\"An√°lise de Sentimentos por T√≥pico\")\n",
        "\n",
        "                        st.sidebar.info(\n",
        "                            \"**Sobre a An√°lise de T√≥picos:**\\n\\n\"\n",
        "                            \"- T√≥picos extra√≠dos com BERTopic.\\n\"\n",
        "                            \"- T√≥pico '-1' agrupa posts considerados outliers.\\n\"\n",
        "                            \"- 'Palavras-Chave' s√£o os termos mais significativos.\\n\"\n",
        "                            \"- Sentimento por T√≥pico √© a distribui√ß√£o percentual dos posts.\"\n",
        "                        )\n",
        "\n",
        "                        st.metric(label=\"Total de T√≥picos Descobertos\", value=len(st.session_state.get('topic_info_df', pd.DataFrame())))\n",
        "\n",
        "                        source_topic_df = st.session_state['topic_info_df'].copy()\n",
        "                        rename_map = {'Topic': 'ID T√≥pico', 'Count': 'N¬∫ Posts', 'Name': 'Palavras-Chave'}\n",
        "                        display_df = source_topic_df.rename(columns={k: v for k, v in rename_map.items() if k in source_topic_df.columns})\n",
        "\n",
        "                        if 'Palavras-Chave' in display_df.columns and 'Name' in source_topic_df.columns:\n",
        "                            display_df['Palavras-Chave'] = display_df['Palavras-Chave'].apply(lambda x: \", \".join(x.split('_')[1:]) if isinstance(x, str) and '_' in x else x)\n",
        "                        \n",
        "                        cols_for_main_display = [col for col in display_df.columns if col not in ['Representation', 'Representative_Docs', 'Representative_Samples']]\n",
        "                        st.dataframe(display_df[cols_for_main_display], use_container_width=True)\n",
        "\n",
        "                        topic_model_instance = st.session_state.get('topic_model_instance')\n",
        "                        if topic_model_instance:\n",
        "                            try:\n",
        "                                num_topics_available = len(st.session_state['topic_info_df'])\n",
        "                                if num_topics_available > 0:\n",
        "                                    with tab2:\n",
        "                                        st.subheader(\"Mapa de Dist√¢ncia Entre T√≥picos\")\n",
        "                                        fig_topics = topic_model_instance.visualize_topics(top_n_topics=num_topics_available, title=\"\")\n",
        "                                        st.plotly_chart(fig_topics, use_container_width=True)\n",
        "\n",
        "                                        with st.expander(\"üó∫Ô∏è O que este Gr√°fico mostra?\"):\n",
        "                                            st.markdown(\"\"\"\n",
        "                                            - **Cada C√≠rculo √© um T√≥pico**: O tamanho indica a frequ√™ncia (n√∫mero de posts).\n",
        "                                            - **Dist√¢ncia**: C√≠rculos pr√≥ximos representam t√≥picos semanticamente similares. C√≠rculos distantes s√£o sobre assuntos diferentes.\n",
        "                                            - **Interatividade**: Clique em um c√≠rculo para ver seus t√≥picos mais relacionados.\n",
        "                                            \"\"\")\n",
        "\n",
        "                                    with tab3:\n",
        "                                        st.subheader(\"Palavras Mais Importantes por T√≥pico\")\n",
        "                                        barchart_height = max(200, (num_topics_available * 3) + 0)\n",
        "                                        fig_barchart = topic_model_instance.visualize_barchart(top_n_topics=num_topics_available, height=barchart_height, n_words=3, title=\"\")\n",
        "                                        st.plotly_chart(fig_barchart, use_container_width=True)\n",
        "                                        \n",
        "                                        with st.expander(\"üìä O que este Gr√°fico mostra?\"):\n",
        "                                            st.markdown(\"\"\"\n",
        "                                            - **Cada Sub-gr√°fico √© um T√≥pico**: Detalha a composi√ß√£o de cada t√≥pico individualmente.\n",
        "                                            - **Comprimento das Barras**: Representa a import√¢ncia de cada palavra para aquele t√≥pico espec√≠fico (score c-TF-IDF), n√£o apenas sua frequ√™ncia geral.\n",
        "                                            \"\"\")\n",
        "                                        topic_model_instance.visualize_hierarchy()\n",
        "\n",
        "                            except Exception as e:\n",
        "                                st.warning(f\"N√£o foi poss√≠vel gerar visualiza√ß√µes dos t√≥picos: {e}\", icon=\"‚ö†Ô∏è\")\n",
        "                with tab4:        \n",
        "                    if st.session_state.get('topics_analyzed', False) and not st.session_state.get('topic_info_df', pd.DataFrame()).empty:\n",
        "                        st.subheader(\"Pesquisar T√≥pico por Palavra-Chave\")\n",
        "                        search_term = st.text_input(\"Digite uma palavra-chave:\", placeholder=\"Ex: economy, trump, brasil\", help=\"Pesquise t√≥picos por palavras-chave. Exemplo: 'economy', 'trump', 'brasil'.\")\n",
        "\n",
        "                        if search_term:\n",
        "                            if 'Palavras-Chave' in display_df.columns:\n",
        "                                results_df = display_df[display_df['Palavras-Chave'].str.contains(search_term, case=False, na=False)]\n",
        "                                if not results_df.empty:\n",
        "                                    search_result_cols = ['ID T√≥pico', 'Palavras-Chave', 'N¬∫ Posts', 'Positive (%)', 'Negative (%)', 'Neutral (%)']\n",
        "                                    final_cols = [col for col in search_result_cols if col in results_df.columns]\n",
        "                                    st.write(f\"Resultados da busca para \\\"{search_term}\\\":\")\n",
        "                                    st.dataframe(results_df[final_cols], use_container_width=True)\n",
        "\n",
        "                                    # Expander com os posts dos t√≥picos encontrados\n",
        "                                    found_topic_ids = results_df['ID T√≥pico'].tolist()\n",
        "                                    posts_in_found_topics = df_collected[df_collected['topic_id'].isin(found_topic_ids)]\n",
        "\n",
        "                                    with st.expander(f\"Ver posts dos t√≥picos encontrados na busca por '{search_term}'\"):\n",
        "                                        if not posts_in_found_topics.empty:\n",
        "                                            posts_to_show = posts_in_found_topics[['text', 'sentiment', 'topic_id']]\n",
        "                                            st.dataframe(posts_to_show, use_container_width=True)\n",
        "                                        else:\n",
        "                                            st.info(\"N√£o foram encontrados posts para os t√≥picos desta busca.\")\n",
        "                                else:\n",
        "                                    st.info(f\"Nenhum t√≥pico encontrado com a palavra-chave \\\"{search_term}\\\".\")\n",
        "\n",
        "                    with tab5:\n",
        "                        if st.session_state.get('topics_analyzed', False) and not df_collected.empty:\n",
        "                            st.subheader(\"Dados Coletados Detalhados\")\n",
        "                            st.dataframe(df_collected, use_container_width=True)\n",
        "\n",
        "\n",
        "        elif st.session_state['collection_ended'] and not st.session_state['data']:\n",
        "            st.warning(\"Nenhum post foi coletado durante o per√≠odo especificado ou que corresponda aos crit√©rios.\", icon=\"‚ö†Ô∏è\")\n",
        "            if st.button(\"Tentar Nova Coleta\", icon=\":material/refresh:\", use_container_width=True):\n",
        "                self._reset_all_states()\n",
        "                st.rerun()\n",
        "\n",
        "\n",
        "    def _reset_all_states(self):\n",
        "        \"\"\"\n",
        "        Fun√ß√£o auxiliar para limpar todos os estados da sess√£o.\n",
        "        \"\"\"\n",
        "        st.session_state.update({\n",
        "            'data': [], 'collection_ended': False, 'collecting': False, 'sentiment_results': [], \n",
        "            'collected_df': pd.DataFrame(), 'collected_df_for_download': pd.DataFrame(),\n",
        "            'stop_event': multiprocessing.Event(), 'data_queue': multiprocessing.Queue(),\n",
        "            'topic_model_instance': None, 'topic_info_df': pd.DataFrame(), 'topics_analyzed': False, \n",
        "            'performing_topic_analysis': False, 'texts_for_topic_analysis': [],\n",
        "            'sentiment_analysis_toast_shown': False, 'topics_analyzed_toast_shown': False\n",
        "        })\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        M√©todo principal que executa o aplicativo.\n",
        "        \"\"\"\n",
        "        st.markdown(f\"<div style='text-align: left;'>{self.bskylogo_svg_template}</div>\", unsafe_allow_html=True)\n",
        "        st.text(\"\")\n",
        "        st.text(\"\")\n",
        "\n",
        "        st.sidebar.markdown(self.bskylogo_svg_template, unsafe_allow_html=True)\n",
        "        st.sidebar.title(\"BskyMood\")\n",
        "        st.sidebar.markdown(\"**Coleta e An√°lise de T√≥picos e Sentimentos no Bluesky**\")\n",
        "\n",
        "        if not st.session_state['collecting'] and not st.session_state['collection_ended']:\n",
        "            st.warning(\"Nenhum post coletado ainda. Clique no bot√£o 'Iniciar Coleta' para come√ßar.\", icon=\":material/warning:\")\n",
        "            st.sidebar.info(\n",
        "                \"**Antes de come√ßar**\\n\\n\"\n",
        "                \"- Selecione um intervalo de coleta e clique em 'Iniciar Coleta'.\\n\"\n",
        "                \"- Intervalos maiores implicam em maior tempo de coleta e processamento.\\n\"\n",
        "                \"- As postagens coletadas podem incluir termos ofensivos ou inadequados.\"\n",
        "            )\n",
        "\n",
        "            st.session_state['collection_duration'] = st.sidebar.slider(\n",
        "                \"Dura√ß√£o da Coleta (segundos)\", min_value=10, max_value=60, value=10, step=5,\n",
        "                help=\"Defina por quanto tempo os posts ser√£o coletados.\"\n",
        "            )\n",
        "\n",
        "            if st.sidebar.button(\"Iniciar Coleta\", icon=\":material/play_circle:\", use_container_width=True, type=\"primary\"):\n",
        "                self._reset_all_states()\n",
        "                st.session_state['collecting'] = True\n",
        "                st.session_state['stop_event'].clear()\n",
        "                st.rerun()\n",
        "\n",
        "        elif st.session_state['collecting']:\n",
        "            self.collect_data()\n",
        "\n",
        "        if not st.session_state['collecting']:\n",
        "            self.display_data()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = BskyDataCollectorApp()\n",
        "    app.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rj3GSLHWxKX"
      },
      "source": [
        "#### **Aten√ß√£o**\n",
        "A c√©lula 5 inicia a execu√ß√£o do aplicativo em uma nova aba do seu navegador. Na p√°gina, insira a senha copiada anteriormente e clique no bot√£o azul para confirmar.\n",
        "\n",
        "O BskyMood ser√° carregado. Aguarde alguns minutos para sua estabiliza√ß√£o antes de come√ßar a us√°-lo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXABiDfHU2WG"
      },
      "outputs": [],
      "source": [
        "# 5. Exposi√ß√£o do localtunnel e Execu√ß√£o do BskyMood\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slbw-i31VBJX"
      },
      "source": [
        "### **Para encerrar a execu√ß√£o, interrompa a c√©lula do Passo 5.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNXbqIHiGi/T03plgCdLjsA",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
